<!DOCTYPE html>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>RNN-电影评论情感分析 | Renne&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="之前本来是打算写第4份作业的，但后来想想还是弄一份好一点的数据集的，不想在编码问题弄这么久。而且，给出完整的代码很没意思，还是能够自己简单写点东西有点意思，主要是想练一下numpy和Python而已。 第一次用pycharm的ipynb的插件，感觉有点麻烦还有点bug，挺不好用的，后面很多东西都直接写进一份.py的文件里算了。 之前在上课的时候是学了RNN和word embedding这样的东西。">
<meta name="keywords" content="机器学习,深度学习,RNN,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN-电影评论情感分析">
<meta property="og:url" content="http://yoursite.com/2020/09/03/2020-09-03-rnn-movie-review-sentiment-classification/index.html">
<meta property="og:site_name" content="Renne&#39;s Blog">
<meta property="og:description" content="之前本来是打算写第4份作业的，但后来想想还是弄一份好一点的数据集的，不想在编码问题弄这么久。而且，给出完整的代码很没意思，还是能够自己简单写点东西有点意思，主要是想练一下numpy和Python而已。 第一次用pycharm的ipynb的插件，感觉有点麻烦还有点bug，挺不好用的，后面很多东西都直接写进一份.py的文件里算了。 之前在上课的时候是学了RNN和word embedding这样的东西。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://img.nga.178.com/attachments/mon_202008/29/f0Q5-7uniZaT3cSsg-fz.jpg">
<meta property="og:updated_time" content="2020-09-03T11:14:41.768Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RNN-电影评论情感分析">
<meta name="twitter:description" content="之前本来是打算写第4份作业的，但后来想想还是弄一份好一点的数据集的，不想在编码问题弄这么久。而且，给出完整的代码很没意思，还是能够自己简单写点东西有点意思，主要是想练一下numpy和Python而已。 第一次用pycharm的ipynb的插件，感觉有点麻烦还有点bug，挺不好用的，后面很多东西都直接写进一份.py的文件里算了。 之前在上课的时候是学了RNN和word embedding这样的东西。">
<meta name="twitter:image" content="https://img.nga.178.com/attachments/mon_202008/29/f0Q5-7uniZaT3cSsg-fz.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Renne&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
	<div id="header-touxiang"></div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Renne&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">built on 2019-06-20</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2020-09-03-rnn-movie-review-sentiment-classification" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/03/2020-09-03-rnn-movie-review-sentiment-classification/" class="article-date">
  <time datetime="2020-09-03T01:43:10.000Z" itemprop="datePublished">2020-09-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RNN-电影评论情感分析
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>之前本来是打算写第4份作业的，但后来想想还是弄一份好一点的数据集的，不想在编码问题弄这么久。而且，给出完整的代码很没意思，还是能够自己简单写点东西有点意思，主要是想练一下numpy和Python而已。</p>
<p>第一次用pycharm的ipynb的插件，感觉有点麻烦还有点bug，挺不好用的，后面很多东西都直接写进一份<code>.py</code>的文件里算了。</p>
<p>之前在上课的时候是学了RNN和word embedding这样的东西。我们都知道word embedding能将onehot key转换为一个vector，而RNN或者一般说LSTM能按顺序处理并记忆下的东西，对付NLP这种顺序相关的运算特别有优势。</p>
<p>本次的实现完全参照<a href="https://blog.csdn.net/zwqjoy/article/details/94750649" target="_blank" rel="noopener">这个博客</a>，也只是有一些部分为了熟练python和numpy自己写了点而已，思路和模型都是一模一样的。</p>
<a id="more"></a>
<h2 id="数据读入及预处理"><a class="markdownIt-Anchor" href="#数据读入及预处理"></a> 数据读入及预处理</h2>
<h3 id="文件读取"><a class="markdownIt-Anchor" href="#文件读取"></a> 文件读取</h3>
<p>读取很简单粗暴，直接将数据读进一个串里面。对于NLP的情景，数据量一般都不会非常大，不像之前摆弄CV的时候那么悲催。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"./data/reviews.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reviews = f.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./data/labels.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    labels = f.read()</span><br></pre></td></tr></table></figure>
<h3 id="评论数据清洗"><a class="markdownIt-Anchor" href="#评论数据清洗"></a> 评论数据清洗</h3>
<p>做法很简单，就是直接将标点符号全部去掉而已，直接用string的库，顺便弄一个包含全部单词的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> string <span class="keyword">import</span> punctuation</span><br><span class="line"></span><br><span class="line">reviews = reviews.lower()</span><br><span class="line">all_text = <span class="string">""</span>.join([c <span class="keyword">for</span> c <span class="keyword">in</span> reviews <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> punctuation])</span><br><span class="line"></span><br><span class="line">reviews_split = all_text.split(<span class="string">'\n'</span>)</span><br><span class="line">words = all_text.split()</span><br></pre></td></tr></table></figure>
<h3 id="label数据处理"><a class="markdownIt-Anchor" href="#label数据处理"></a> label数据处理</h3>
<p>因为label是以<code>positive</code>和<code>negetive</code>区分的，而我们用于情感分析还是用数字零和一比较好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labels_split = labels.split(<span class="string">'\n'</span>)</span><br><span class="line">labels_split_np = [<span class="number">1.0</span> <span class="keyword">if</span> w == <span class="string">"positive"</span> <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> w <span class="keyword">in</span> labels_split]</span><br></pre></td></tr></table></figure>
<h3 id="单词标号"><a class="markdownIt-Anchor" href="#单词标号"></a> 单词标号</h3>
<p>就是标个号而已，直接加上去，按照出现次数排序编号。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单词初步encode，其实就是编个号而已</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">counts = Counter(words)</span><br><span class="line">vocab = sorted(counts, key=counts.get, reverse=<span class="literal">True</span>)</span><br><span class="line">vocab_to_int = &#123;word: index <span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(vocab, <span class="number">1</span>)&#125;</span><br><span class="line"></span><br><span class="line">reviews_int = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> reviews_split:</span><br><span class="line">    sentence_split = sentence.split()</span><br><span class="line">    sentence_int = [vocab_to_int[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence_split]</span><br><span class="line">    reviews_int.append(sentence_int)</span><br></pre></td></tr></table></figure>
<h2 id="样本划分和数据格式化"><a class="markdownIt-Anchor" href="#样本划分和数据格式化"></a> 样本划分和数据格式化</h2>
<h3 id="句子截取"><a class="markdownIt-Anchor" href="#句子截取"></a> 句子截取</h3>
<p>原本是想按照之前的博客上写的截取长度为200的句子的，结果统计了一下一看，长度大于200的大概有20%的数据。。。</p>
<p>所以就想搞成800，这样只浪费2%的数据。我设定的句子vector的输出是句子长度的2倍，所以弄800就直接爆炸了，然后就折衷换成400了。其实现在回想，应该不改变句子vector长度，只改变句子截取的长度也是可以的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统一句子长度为400，p.s.本来想800的，结果显存不够了。。</span></span><br><span class="line">sentence_len = <span class="number">400</span></span><br><span class="line"><span class="comment"># oneTcount = 0</span></span><br><span class="line"><span class="comment"># for review in reviews_int:</span></span><br><span class="line"><span class="comment">#    if len(review) &gt; 800:</span></span><br><span class="line"><span class="comment">#        oneTcount += 1</span></span><br><span class="line"><span class="comment"># print(f"&#123;oneTcount&#125;/&#123;len(reviews_int)&#125;")</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, _ <span class="keyword">in</span> enumerate(reviews_int):</span><br><span class="line">    <span class="keyword">if</span> len(reviews_int[index]) &gt;= sentence_len:</span><br><span class="line">        reviews_int[index] = reviews_int[index][:sentence_len]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        len_seq = len(reviews_int[index])</span><br><span class="line">        <span class="comment"># 记住extend和append的区别</span></span><br><span class="line">        reviews_int[index].extend([<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(sentence_len - len_seq)])</span><br><span class="line">    reviews_int[index] = torch.tensor(reviews_int[index])</span><br></pre></td></tr></table></figure>
<h3 id="数据划分"><a class="markdownIt-Anchor" href="#数据划分"></a> 数据划分</h3>
<p>这里的数据集就按照之前问别人论文里面常用的那样，二八分。由于后面设置了一些隐藏层，所以就保证每个batch的数量全部一样，不要说最后一个batch少了几个，这样比较好操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集训练集划分，总共有25001条数据，为了方便取10为1batch，82开，忽略最后一笔</span></span><br><span class="line">split_frac = <span class="number">0.8</span></span><br><span class="line">split_idx = int(len(reviews_int) * split_frac)</span><br><span class="line">train_X, train_y = reviews_int[:split_idx], labels_split_np[:split_idx]</span><br><span class="line">test_X, test_y = reviews_int[split_idx:len(reviews_int) - <span class="number">1</span>], labels_split_np[split_idx:len(labels_split_np) - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="dataset和dataloader制作"><a class="markdownIt-Anchor" href="#dataset和dataloader制作"></a> Dataset和DataLoader制作</h3>
<p>这部分挺坑的。</p>
<p>其一是显存，我那1650卡总算感到吃力了，才4G的显存，不是直接爆CUDA out of memory，就是直接退出。因为pytorch在执行cuda的时候所申请的显存，并不止我们在任务管理器看到的那些，还包括了一些临时的显存。如果是因为临时的显存爆炸，会直接退出，爆<code>-1073740791</code>的返回代码。所以调节显存的两个点就是batch的大小以及模型的大小。其中batch的大小比较关键，毕竟里面每次运算的所申请的显存都和batch大小有线性的关系。我的电脑得把batch调到10才可以。另外一个调整的方法就是缩小网络了，缩小网络作用相比之下显存变化较小，且会降低准确率。所以还是调batch用时间换空间，这和以前的做法反过来。。</p>
<p>其二是DataLoader的读出机制，假如将[1,2],[3,4],[5,6]，用dataloader合并成batch，得到的结果会是[1,2,3],[4,5,6]。所以我们在输入到lstm时，要注意启用<code>batch_first</code>，不然他会以为输入的是[1,2],[3,4],[5,6]那样的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset和dataloader制作，用于batch数据</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CommentDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.data = X</span><br><span class="line">        self.label = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.label <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> self.data[idx]</span><br><span class="line">        <span class="keyword">return</span> self.data[idx], self.label[idx]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dataset = CommentDataset(X=train_X, y=train_y)</span><br><span class="line">test_dataset = CommentDataset(X=test_X, y=test_y)</span><br><span class="line"></span><br><span class="line">train_loader = data.DataLoader(dataset=train_dataset,</span><br><span class="line">                               batch_size=<span class="number">10</span>,</span><br><span class="line">                               shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = data.DataLoader(dataset=test_dataset,</span><br><span class="line">                              batch_size=<span class="number">10</span>,</span><br><span class="line">                              shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#%%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loader测试</span></span><br><span class="line"><span class="comment"># count = 0</span></span><br><span class="line"><span class="comment"># for data_batch_X,data_batch_y in test_loader:</span></span><br><span class="line"><span class="comment">#    if count == 0:</span></span><br><span class="line"><span class="comment">#        batch_size_x = len(data_batch_X)</span></span><br><span class="line"><span class="comment">#        batch_size_y = len(data_batch_y)</span></span><br><span class="line"><span class="comment">#        print(batch_size_y)</span></span><br><span class="line"><span class="comment">#        print(batch_size_x)</span></span><br><span class="line"><span class="comment">#    count += 1</span></span><br><span class="line"><span class="comment"># 假如将[1,2],[3,4],[5,6]，用dataloader合并成batch，得到的结果会是[1,2,3],[4,5,6]</span></span><br><span class="line"><span class="comment"># 因此len(batch_size_x)会是800，而batch_size_y是128</span></span><br></pre></td></tr></table></figure>
<h2 id="模型建立"><a class="markdownIt-Anchor" href="#模型建立"></a> 模型建立</h2>
<p>我的模型很简单，直接抄别人的，要提出新模型至少现在还做不到。代码也大部分抄的，像隐藏层这些骚操作我还第一次见到，所以就分析一下别人的东西吧。</p>
<p><a href="https://img-blog.csdnimg.cn/20190705191637908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p3cWpveQ==,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener">情感分析模型</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LSTM_Moudle(</span><br><span class="line">  (embedding): Embedding(74073, 800)</span><br><span class="line">  (lstm): LSTM(800, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)</span><br><span class="line">  (drop): Dropout(p=0.3, inplace=False)</span><br><span class="line">  (regression): Linear(in_features=512, out_features=1, bias=True)</span><br><span class="line">  (sig): Sigmoid()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>首先是embedding layer，它只是用于每一个词转换成词向量，然后合并后就得到了句向量。</p>
<p>接着堆叠到两层的lstm按顺序分析每个词向量，我们都知道LSTM具有记忆功能，它能够记录下句子的顺序信息。在LSTM的选项中，我们可以开启双向的LSTM，这样能够对某个词得到前后两个不同顺序信息。也因此它最终会有两个隐藏层的vector，因为前序和后序是分别进行的，所以后面的hidden表示的就是这个意思，如果选择双向，就需要将数据扩展到hidden上，然后再接下去下一步的全连接操作。</p>
<p>在使用Linear进行全连接之前，可以弄个dropout稍稍提高一下效率，也可以简化一下网络。在linear上布置一个0.3概率的dropout，也就是有0.3的概率丢掉某条全连接的边，在eval时会取消这个模式。最后由于是个二分类问题，所以用个Sigmoid判断一下就好，就是个零一问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Moudle</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional=True, drop_prob=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(LSTM_Moudle, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line"></span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># batch_first 每一列一个句子，根我们用dataLoader里表示的一样</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=bidirectional,</span><br><span class="line">                            batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.drop = nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.regression = nn.Linear(hidden_dim * <span class="number">2</span>, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.regression = nn.Linear(hidden_dim, output_size)</span><br><span class="line">        self.sig = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, hidden)</span>:</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        embed = self.embedding(x)</span><br><span class="line">        lstm_out, hidden = self.lstm(embed, hidden)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            lstm_out = lstm_out.contiguous().view(<span class="number">-1</span>, self.hidden_dim * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lstm_out = lstm_out.contiguous().view(<span class="number">-1</span>, self.hidden_dim)</span><br><span class="line">        output = self.drop(lstm_out)</span><br><span class="line">        output = self.regression(output)</span><br><span class="line"></span><br><span class="line">        sig_out = self.sig(output)</span><br><span class="line">        sig_out = sig_out.view(batch_size, <span class="number">-1</span>)</span><br><span class="line">        sig_out = sig_out[:, <span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> sig_out, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        weight = next(self.parameters()).data</span><br><span class="line">        number = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            number = <span class="number">2</span></span><br><span class="line">        hidden = (weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().cuda(),</span><br><span class="line">                  weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().cuda())</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line"></span><br><span class="line">vocab_size = len(vocab_to_int) + <span class="number">1</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line">embedding_dim = sentence_len * <span class="number">2</span></span><br><span class="line">hidden_dim = <span class="number">256</span></span><br><span class="line">n_layers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">model = LSTM_Moudle(vocab_size, output_size, embedding_dim=embedding_dim,</span><br><span class="line">                    hidden_dim=hidden_dim, n_layers=n_layers)</span><br><span class="line"></span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练和验证"><a class="markdownIt-Anchor" href="#模型训练和验证"></a> 模型训练和验证</h2>
<p>模型的训练也是中规中矩，像以前学的一样，每个epoch统计一次training loss和testing loss，看一下结果怎么样就可以了。最终的准确度有80左右，还算很不错了，在这种环境下。调了很多次才跑成功，经常跑到一般就因为爆显存挂掉了。</p>
<p>这部分比较重要的是变量的位置，它是在cpu里面还是在gpu里面，其它都差不多。里面有个与以前的不同点，上课的时候也分析过RNN的一些缺点，它很容易会导致梯度爆炸或者梯度消失，而LSTM虽然引入forget缓解梯度消失，但梯度爆炸还是会存在的。所以我们在optimizer执行前，我们把梯度卡在5这里，大于5的就给他剪个平头就好了。</p>
<p>可能原先博主的电脑比较好，他可以执行到一部分就统计一下Loss，但这样占用显存会比较大。我还是采取占用较小的、又比较好写的方法，每次扫完一个epoch才计算一次验证集的数据。在最后计算正确率的时候，由于对tensor不太熟，所以得先把predict结果和label返回到cpu上，转成numpy数组才进行判断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">loss_function = nn.BCELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">clip = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"training sentences:"</span>,len(train_X))</span><br><span class="line">print(<span class="string">"training labels  :"</span>,len(train_y))</span><br><span class="line">print(<span class="string">"testing sentences:"</span>,len(test_X))</span><br><span class="line">print(<span class="string">"testing labels   :"</span>,len(test_y))</span><br><span class="line"></span><br><span class="line">training_losses = []</span><br><span class="line">val_losses = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    hidden = model.init_hidden(batch_size=<span class="number">10</span>)</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"epoch <span class="subst">&#123;epoch&#125;</span> started:"</span>)</span><br><span class="line">    training_losses_epoch = []</span><br><span class="line">    <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        inputs = inputs.cuda()</span><br><span class="line">        labels = labels.cuda()</span><br><span class="line">        hidden = tuple([each.data <span class="keyword">for</span> each <span class="keyword">in</span> hidden])</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(inputs, hidden)</span><br><span class="line"></span><br><span class="line">        loss = loss_function(output, labels.float())</span><br><span class="line">        training_losses_epoch.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), <span class="number">5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;..."</span>.format(epoch + <span class="number">1</span>, epochs),</span><br><span class="line">          <span class="string">"Training Loss: &#123;:.6f&#125;..."</span>.format(np.mean(training_losses_epoch)))</span><br><span class="line">    training_losses.append(np.mean(training_losses_epoch))</span><br><span class="line"></span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    val_h = model.init_hidden(<span class="number">10</span>)</span><br><span class="line">    val_losses_epoch = []</span><br><span class="line">    num_correct_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> val_inputs, val_labels <span class="keyword">in</span> test_loader:</span><br><span class="line">            val_h = tuple([each.data <span class="keyword">for</span> each <span class="keyword">in</span> val_h])</span><br><span class="line">            val_inputs = val_inputs.cuda()</span><br><span class="line">            val_labels = val_labels.cuda()</span><br><span class="line">            output, val_h = model(val_inputs, val_h)</span><br><span class="line">            val_loss = loss_function(output, val_labels.float())</span><br><span class="line">            val_losses_epoch.append(val_loss.item())</span><br><span class="line"></span><br><span class="line">            pred = torch.round(output)</span><br><span class="line">            val_labels = val_labels.cpu()</span><br><span class="line">            pred = pred.cpu()</span><br><span class="line">            num_correct = np.sum(pred.numpy() == val_labels.numpy())</span><br><span class="line">            num_correct_sum += num_correct</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;..."</span>.format(epoch + <span class="number">1</span>, epochs),</span><br><span class="line">          <span class="string">"Val Loss: &#123;:.6f&#125;"</span>.format(np.mean(val_losses_epoch)),</span><br><span class="line">          <span class="string">"Correct Rate: &#123;&#125;% (&#123;&#125;/&#123;&#125;)"</span>.format(int(num_correct_sum/len(test_y)*<span class="number">10000</span>)/<span class="number">100</span>,num_correct_sum, len(test_y)))</span><br><span class="line">    val_losses.append(np.mean(val_losses_epoch))</span><br><span class="line">    model.train()</span><br></pre></td></tr></table></figure>
<p>这份代码在1650的卡上跑了挺久的，最终也有80%左右的正确率。但最终结果没能留下了</p>
<h2 id="线上运行"><a class="markdownIt-Anchor" href="#线上运行"></a> 线上运行</h2>
<p>后来，稍稍改了改代码，将这些数据放极客云上3块一小时租的带1080卡的服务器上运行，得到了相似的结果。更改的部分基本都是batch数量、epochs、句子长度。</p>
<p>机子的配置是1080卡G4560的处理器16G内存，典型的高卡低U，幸好不是在玩游戏。。CPU的操作和文本处理操作非常慢，但跑dl就快很多。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./data/reviews.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reviews = f.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./data/labels.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    labels = f.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理评论数据</span></span><br><span class="line"><span class="keyword">from</span> string <span class="keyword">import</span> punctuation</span><br><span class="line"></span><br><span class="line">reviews = reviews.lower()</span><br><span class="line">all_text = <span class="string">""</span>.join([c <span class="keyword">for</span> c <span class="keyword">in</span> reviews <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> punctuation])</span><br><span class="line"></span><br><span class="line">reviews_split = all_text.split(<span class="string">'\n'</span>)</span><br><span class="line">words = all_text.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line">labels_split = labels.split(<span class="string">'\n'</span>)</span><br><span class="line">labels_split_np = [<span class="number">1.0</span> <span class="keyword">if</span> w == <span class="string">"positive"</span> <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> w <span class="keyword">in</span> labels_split]</span><br><span class="line"><span class="comment"># print(labels_split_np)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单词初步encode，其实就是编个号而已</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">counts = Counter(words)</span><br><span class="line">vocab = sorted(counts, key=counts.get, reverse=<span class="literal">True</span>)</span><br><span class="line">vocab_to_int = &#123;word: index <span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(vocab, <span class="number">1</span>)&#125;</span><br><span class="line"></span><br><span class="line">reviews_int = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> reviews_split:</span><br><span class="line">    sentence_split = sentence.split()</span><br><span class="line">    sentence_int = [vocab_to_int[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence_split]</span><br><span class="line">    reviews_int.append(sentence_int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统一句子长度为200，p.s.本来想800的，结果显存不够了。。</span></span><br><span class="line">sentence_len = <span class="number">400</span></span><br><span class="line"><span class="comment"># oneTcount = 0</span></span><br><span class="line"><span class="comment"># for review in reviews_int:</span></span><br><span class="line"><span class="comment">#    if len(review) &gt; 800:</span></span><br><span class="line"><span class="comment">#        oneTcount += 1</span></span><br><span class="line"><span class="comment"># print(f"&#123;oneTcount&#125;/&#123;len(reviews_int)&#125;")</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, _ <span class="keyword">in</span> enumerate(reviews_int):</span><br><span class="line">    <span class="keyword">if</span> len(reviews_int[index]) &gt;= sentence_len:</span><br><span class="line">        reviews_int[index] = reviews_int[index][:sentence_len]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        len_seq = len(reviews_int[index])</span><br><span class="line">        <span class="comment"># 记住extend和append的区别</span></span><br><span class="line">        reviews_int[index].extend([<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(sentence_len - len_seq)])</span><br><span class="line">    reviews_int[index] = torch.tensor(reviews_int[index])</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集训练集划分，总共有25001条数据，为了方便取25为1batch，28开，忽略最后一笔</span></span><br><span class="line">split_frac = <span class="number">0.8</span></span><br><span class="line">split_idx = int(len(reviews_int) * split_frac)</span><br><span class="line">train_X, train_y = reviews_int[:split_idx], labels_split_np[:split_idx]</span><br><span class="line">test_X, test_y = reviews_int[split_idx:len(reviews_int) - <span class="number">1</span>], labels_split_np[split_idx:len(labels_split_np) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset和dataloader制作，用于batch数据</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CommentDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.data = X</span><br><span class="line">        self.label = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.label <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> self.data[idx]</span><br><span class="line">        <span class="keyword">return</span> self.data[idx], self.label[idx]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dataset = CommentDataset(X=train_X, y=train_y)</span><br><span class="line">test_dataset = CommentDataset(X=test_X, y=test_y)</span><br><span class="line"></span><br><span class="line">train_loader = data.DataLoader(dataset=train_dataset,</span><br><span class="line">                               batch_size=<span class="number">100</span>,</span><br><span class="line">                               shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = data.DataLoader(dataset=test_dataset,</span><br><span class="line">                              batch_size=<span class="number">100</span>,</span><br><span class="line">                              shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loader测试</span></span><br><span class="line"><span class="comment"># count = 0</span></span><br><span class="line"><span class="comment"># for data_batch_X,data_batch_y in test_loader:</span></span><br><span class="line"><span class="comment">#    if count == 0:</span></span><br><span class="line"><span class="comment">#        batch_size_x = len(data_batch_X)</span></span><br><span class="line"><span class="comment">#        batch_size_y = len(data_batch_y)</span></span><br><span class="line"><span class="comment">#        print(batch_size_y)</span></span><br><span class="line"><span class="comment">#        print(batch_size_x)</span></span><br><span class="line"><span class="comment">#    count += 1</span></span><br><span class="line"><span class="comment"># 假如将[1,2],[3,4],[5,6]，用dataloader合并成batch，得到的结果会是[1,2,3],[4,5,6]</span></span><br><span class="line"><span class="comment"># 因此len(batch_size_x)会是800，而batch_size_y是128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型建立</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">train_on_gpu = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span>) <span class="keyword">if</span> train_on_gpu <span class="keyword">else</span> torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"><span class="keyword">if</span> train_on_gpu:</span><br><span class="line">    print(<span class="string">"training on GPU"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"training on CPU"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Moudle</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional=True, drop_prob=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(LSTM_Moudle, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line"></span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># batch_first 每一列一个句子，根我们用dataLoader里表示的一样</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=bidirectional,</span><br><span class="line">                            batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.drop = nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.regression = nn.Linear(hidden_dim * <span class="number">2</span>, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.regression = nn.Linear(hidden_dim, output_size)</span><br><span class="line">        self.sig = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, hidden)</span>:</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        embed = self.embedding(x)</span><br><span class="line">        lstm_out, hidden = self.lstm(embed, hidden)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            lstm_out = lstm_out.contiguous().view(<span class="number">-1</span>, self.hidden_dim * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lstm_out = lstm_out.contiguous().view(<span class="number">-1</span>, self.hidden_dim)</span><br><span class="line">        output = self.drop(lstm_out)</span><br><span class="line">        output = self.regression(output)</span><br><span class="line"></span><br><span class="line">        sig_out = self.sig(output)</span><br><span class="line">        sig_out = sig_out.view(batch_size, <span class="number">-1</span>)</span><br><span class="line">        sig_out = sig_out[:, <span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> sig_out, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        weight = next(self.parameters()).data</span><br><span class="line">        number = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            number = <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> train_on_gpu:</span><br><span class="line">            hidden = (weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().cuda(),</span><br><span class="line">                      weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_().cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = (weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_(),</span><br><span class="line">                      weight.new(self.n_layers * number, batch_size, self.hidden_dim).zero_())</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line">vocab_size = len(vocab_to_int) + <span class="number">1</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line">embedding_dim = sentence_len * <span class="number">2</span></span><br><span class="line">hidden_dim = <span class="number">256</span></span><br><span class="line">n_layers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">model = LSTM_Moudle(vocab_size, output_size, embedding_dim=embedding_dim,</span><br><span class="line">                    hidden_dim=hidden_dim, n_layers=n_layers)</span><br><span class="line"></span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">loss_function = nn.BCELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">clip = <span class="number">5</span></span><br><span class="line">log_every = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> train_on_gpu:</span><br><span class="line">    model.cuda()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"training sentences:"</span>, len(train_X))</span><br><span class="line">print(<span class="string">"training labels  :"</span>, len(train_y))</span><br><span class="line">print(<span class="string">"testing sentences:"</span>, len(test_X))</span><br><span class="line">print(<span class="string">"testing labels   :"</span>, len(test_y))</span><br><span class="line"></span><br><span class="line">training_losses = []</span><br><span class="line">val_losses = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    hidden = model.init_hidden(batch_size=<span class="number">100</span>)</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"epoch <span class="subst">&#123;epoch&#125;</span> started:"</span>)</span><br><span class="line">    training_losses_epoch = []</span><br><span class="line">    <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="keyword">if</span> train_on_gpu:</span><br><span class="line">            inputs = inputs.cuda()</span><br><span class="line">            labels = labels.cuda()</span><br><span class="line">        hidden = tuple([each.data <span class="keyword">for</span> each <span class="keyword">in</span> hidden])</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(inputs, hidden)</span><br><span class="line"></span><br><span class="line">        loss = loss_function(output, labels.float())</span><br><span class="line">        training_losses_epoch.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), <span class="number">100</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;..."</span>.format(epoch + <span class="number">1</span>, epochs),</span><br><span class="line">                  <span class="string">"Step: &#123;&#125;/&#123;&#125;..."</span>.format(i + <span class="number">1</span>, int(len(train_X) / <span class="number">100</span>)),</span><br><span class="line">                  <span class="string">"Training Loss: &#123;:.6f&#125;..."</span>.format(np.mean(training_losses_epoch)))</span><br><span class="line">        <span class="keyword">if</span> train_on_gpu:</span><br><span class="line">            torch.cuda.empty_cache()</span><br><span class="line">    training_losses.append(np.mean(training_losses_epoch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    val_h = model.init_hidden(<span class="number">100</span>)</span><br><span class="line">    val_losses_epoch = []</span><br><span class="line">    num_correct_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> val_inputs, val_labels <span class="keyword">in</span> test_loader:</span><br><span class="line">            val_h = tuple([each.data <span class="keyword">for</span> each <span class="keyword">in</span> val_h])</span><br><span class="line">            val_inputs = val_inputs.cuda()</span><br><span class="line">            val_labels = val_labels.cuda()</span><br><span class="line">            output, val_h = model(val_inputs, val_h)</span><br><span class="line">            val_loss = loss_function(output, val_labels.float())</span><br><span class="line">            val_losses_epoch.append(val_loss.item())</span><br><span class="line"></span><br><span class="line">            pred = torch.round(output)</span><br><span class="line">            val_labels = val_labels.cpu()</span><br><span class="line">            pred = pred.cpu()</span><br><span class="line">            num_correct = np.sum(pred.numpy() == val_labels.numpy())</span><br><span class="line">            num_correct_sum += num_correct</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;..."</span>.format(epoch + <span class="number">1</span>, epochs),</span><br><span class="line">          <span class="string">"Val Loss: &#123;:.6f&#125;"</span>.format(np.mean(val_losses_epoch)),</span><br><span class="line">          <span class="string">"Correct Rate: &#123;&#125;% (&#123;&#125;/&#123;&#125;)"</span>.format(int(num_correct_sum / len(test_y) * <span class="number">10000</span>) / <span class="number">100</span>, num_correct_sum,</span><br><span class="line">                                             len(test_y)))</span><br><span class="line">    val_losses.append(np.mean(val_losses_epoch))</span><br><span class="line">    model.train()</span><br></pre></td></tr></table></figure>
<p>完整结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line">python e.py</span><br><span class="line">training on GPU</span><br><span class="line">LSTM_Moudle(</span><br><span class="line">  (embedding): Embedding(74073, 800)</span><br><span class="line">  (lstm): LSTM(800, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)</span><br><span class="line">  (drop): Dropout(p=0.3, inplace=False)</span><br><span class="line">  (regression): Linear(in_features=512, out_features=1, bias=True)</span><br><span class="line">  (sig): Sigmoid()</span><br><span class="line">)</span><br><span class="line">training sentences: 20000</span><br><span class="line">training labels  : 20000</span><br><span class="line">testing sentences: 5000</span><br><span class="line">testing labels   : 5000</span><br><span class="line">epoch 0 started:</span><br><span class="line">Epoch: 1/20... Step: 25/200... Training Loss: 0.705188...</span><br><span class="line">Epoch: 1/20... Step: 50/200... Training Loss: 0.700534...</span><br><span class="line">Epoch: 1/20... Step: 75/200... Training Loss: 0.698390...</span><br><span class="line">Epoch: 1/20... Step: 100/200... Training Loss: 0.697425...</span><br><span class="line">Epoch: 1/20... Step: 125/200... Training Loss: 0.696700...</span><br><span class="line">Epoch: 1/20... Step: 150/200... Training Loss: 0.696449...</span><br><span class="line">Epoch: 1/20... Step: 175/200... Training Loss: 0.695814...</span><br><span class="line">Epoch: 1/20... Step: 200/200... Training Loss: 0.695613...</span><br><span class="line">Epoch: 1/20... Val Loss: 0.692051 Correct Rate: 50.8% (2540/5000)</span><br><span class="line">epoch 1 started:</span><br><span class="line">Epoch: 2/20... Step: 25/200... Training Loss: 0.684146...</span><br><span class="line">Epoch: 2/20... Step: 50/200... Training Loss: 0.684491...</span><br><span class="line">Epoch: 2/20... Step: 75/200... Training Loss: 0.683990...</span><br><span class="line">Epoch: 2/20... Step: 100/200... Training Loss: 0.683713...</span><br><span class="line">Epoch: 2/20... Step: 125/200... Training Loss: 0.683137...</span><br><span class="line">Epoch: 2/20... Step: 150/200... Training Loss: 0.682601...</span><br><span class="line">Epoch: 2/20... Step: 175/200... Training Loss: 0.682178...</span><br><span class="line">Epoch: 2/20... Step: 200/200... Training Loss: 0.682095...</span><br><span class="line">Epoch: 2/20... Val Loss: 0.697679 Correct Rate: 51.55% (2578/5000)</span><br><span class="line">epoch 2 started:</span><br><span class="line">Epoch: 3/20... Step: 25/200... Training Loss: 0.637560...</span><br><span class="line">Epoch: 3/20... Step: 50/200... Training Loss: 0.632466...</span><br><span class="line">Epoch: 3/20... Step: 75/200... Training Loss: 0.633894...</span><br><span class="line">Epoch: 3/20... Step: 100/200... Training Loss: 0.634465...</span><br><span class="line">Epoch: 3/20... Step: 125/200... Training Loss: 0.634200...</span><br><span class="line">Epoch: 3/20... Step: 150/200... Training Loss: 0.635852...</span><br><span class="line">Epoch: 3/20... Step: 175/200... Training Loss: 0.636478...</span><br><span class="line">Epoch: 3/20... Step: 200/200... Training Loss: 0.637850...</span><br><span class="line">Epoch: 3/20... Val Loss: 0.726251 Correct Rate: 51.84% (2592/5000)</span><br><span class="line">epoch 3 started:</span><br><span class="line">Epoch: 4/20... Step: 25/200... Training Loss: 0.619874...</span><br><span class="line">Epoch: 4/20... Step: 50/200... Training Loss: 0.611532...</span><br><span class="line">Epoch: 4/20... Step: 75/200... Training Loss: 0.608977...</span><br><span class="line">Epoch: 4/20... Step: 100/200... Training Loss: 0.607224...</span><br><span class="line">Epoch: 4/20... Step: 125/200... Training Loss: 0.606163...</span><br><span class="line">Epoch: 4/20... Step: 150/200... Training Loss: 0.602905...</span><br><span class="line">Epoch: 4/20... Step: 175/200... Training Loss: 0.604077...</span><br><span class="line">Epoch: 4/20... Step: 200/200... Training Loss: 0.604815...</span><br><span class="line">Epoch: 4/20... Val Loss: 0.816328 Correct Rate: 51.95% (2598/5000)</span><br><span class="line">epoch 4 started:</span><br><span class="line">Epoch: 5/20... Step: 25/200... Training Loss: 0.596018...</span><br><span class="line">Epoch: 5/20... Step: 50/200... Training Loss: 0.599644...</span><br><span class="line">Epoch: 5/20... Step: 75/200... Training Loss: 0.600368...</span><br><span class="line">Epoch: 5/20... Step: 100/200... Training Loss: 0.593245...</span><br><span class="line">Epoch: 5/20... Step: 125/200... Training Loss: 0.582259...</span><br><span class="line">Epoch: 5/20... Step: 150/200... Training Loss: 0.575478...</span><br><span class="line">Epoch: 5/20... Step: 175/200... Training Loss: 0.571676...</span><br><span class="line">Epoch: 5/20... Step: 200/200... Training Loss: 0.564770...</span><br><span class="line">Epoch: 5/20... Val Loss: 0.686458 Correct Rate: 72.06% (3603/5000)</span><br><span class="line">epoch 5 started:</span><br><span class="line">Epoch: 6/20... Step: 25/200... Training Loss: 0.460062...</span><br><span class="line">Epoch: 6/20... Step: 50/200... Training Loss: 0.441465...</span><br><span class="line">Epoch: 6/20... Step: 75/200... Training Loss: 0.443974...</span><br><span class="line">Epoch: 6/20... Step: 100/200... Training Loss: 0.441138...</span><br><span class="line">Epoch: 6/20... Step: 125/200... Training Loss: 0.437420...</span><br><span class="line">Epoch: 6/20... Step: 150/200... Training Loss: 0.437151...</span><br><span class="line">Epoch: 6/20... Step: 175/200... Training Loss: 0.431144...</span><br><span class="line">Epoch: 6/20... Step: 200/200... Training Loss: 0.428100...</span><br><span class="line">Epoch: 6/20... Val Loss: 0.569172 Correct Rate: 77.2% (3860/5000)</span><br><span class="line">epoch 6 started:</span><br><span class="line">Epoch: 7/20... Step: 25/200... Training Loss: 0.316273...</span><br><span class="line">Epoch: 7/20... Step: 50/200... Training Loss: 0.300038...</span><br><span class="line">Epoch: 7/20... Step: 75/200... Training Loss: 0.295198...</span><br><span class="line">Epoch: 7/20... Step: 100/200... Training Loss: 0.304194...</span><br><span class="line">Epoch: 7/20... Step: 125/200... Training Loss: 0.301898...</span><br><span class="line">Epoch: 7/20... Step: 150/200... Training Loss: 0.301596...</span><br><span class="line">Epoch: 7/20... Step: 175/200... Training Loss: 0.298586...</span><br><span class="line">Epoch: 7/20... Step: 200/200... Training Loss: 0.292852...</span><br><span class="line">Epoch: 7/20... Val Loss: 0.606809 Correct Rate: 78.97% (3949/5000)</span><br><span class="line">epoch 7 started:</span><br><span class="line">Epoch: 8/20... Step: 25/200... Training Loss: 0.200525...</span><br><span class="line">Epoch: 8/20... Step: 50/200... Training Loss: 0.195422...</span><br><span class="line">Epoch: 8/20... Step: 75/200... Training Loss: 0.198526...</span><br><span class="line">Epoch: 8/20... Step: 100/200... Training Loss: 0.192401...</span><br><span class="line">Epoch: 8/20... Step: 125/200... Training Loss: 0.187922...</span><br><span class="line">Epoch: 8/20... Step: 150/200... Training Loss: 0.192543...</span><br><span class="line">Epoch: 8/20... Step: 175/200... Training Loss: 0.192555...</span><br><span class="line">Epoch: 8/20... Step: 200/200... Training Loss: 0.191644...</span><br><span class="line">Epoch: 8/20... Val Loss: 0.577024 Correct Rate: 80.58% (4029/5000)</span><br><span class="line">epoch 8 started:</span><br><span class="line">Epoch: 9/20... Step: 25/200... Training Loss: 0.136661...</span><br><span class="line">Epoch: 9/20... Step: 50/200... Training Loss: 0.116079...</span><br><span class="line">Epoch: 9/20... Step: 75/200... Training Loss: 0.122994...</span><br><span class="line">Epoch: 9/20... Step: 100/200... Training Loss: 0.125573...</span><br><span class="line">Epoch: 9/20... Step: 125/200... Training Loss: 0.126975...</span><br><span class="line">Epoch: 9/20... Step: 150/200... Training Loss: 0.129067...</span><br><span class="line">Epoch: 9/20... Step: 175/200... Training Loss: 0.130083...</span><br><span class="line">Epoch: 9/20... Step: 200/200... Training Loss: 0.127143...</span><br><span class="line">Epoch: 9/20... Val Loss: 0.667294 Correct Rate: 80.0% (4000/5000)</span><br><span class="line">epoch 9 started:</span><br><span class="line">Epoch: 10/20... Step: 25/200... Training Loss: 0.083172...</span><br><span class="line">Epoch: 10/20... Step: 50/200... Training Loss: 0.083467...</span><br><span class="line">Epoch: 10/20... Step: 75/200... Training Loss: 0.089386...</span><br><span class="line">Epoch: 10/20... Step: 100/200... Training Loss: 0.091425...</span><br><span class="line">Epoch: 10/20... Step: 125/200... Training Loss: 0.096092...</span><br><span class="line">Epoch: 10/20... Step: 150/200... Training Loss: 0.097790...</span><br><span class="line">Epoch: 10/20... Step: 175/200... Training Loss: 0.096497...</span><br><span class="line">Epoch: 10/20... Step: 200/200... Training Loss: 0.094921...</span><br><span class="line">Epoch: 10/20... Val Loss: 0.682020 Correct Rate: 79.92% (3996/5000)</span><br><span class="line">epoch 10 started:</span><br><span class="line">Epoch: 11/20... Step: 25/200... Training Loss: 0.076430...</span><br><span class="line">Epoch: 11/20... Step: 50/200... Training Loss: 0.076674...</span><br><span class="line">Epoch: 11/20... Step: 75/200... Training Loss: 0.073407...</span><br><span class="line">Epoch: 11/20... Step: 100/200... Training Loss: 0.070446...</span><br><span class="line">Epoch: 11/20... Step: 125/200... Training Loss: 0.071163...</span><br><span class="line">Epoch: 11/20... Step: 150/200... Training Loss: 0.071040...</span><br><span class="line">Epoch: 11/20... Step: 175/200... Training Loss: 0.073278...</span><br><span class="line">Epoch: 11/20... Step: 200/200... Training Loss: 0.074396...</span><br><span class="line">Epoch: 11/20... Val Loss: 0.691229 Correct Rate: 80.3% (4015/5000)</span><br><span class="line">epoch 11 started:</span><br><span class="line">Epoch: 12/20... Step: 25/200... Training Loss: 0.049907...</span><br><span class="line">Epoch: 12/20... Step: 50/200... Training Loss: 0.056414...</span><br><span class="line">Epoch: 12/20... Step: 75/200... Training Loss: 0.060522...</span><br><span class="line">Epoch: 12/20... Step: 100/200... Training Loss: 0.057298...</span><br><span class="line">Epoch: 12/20... Step: 125/200... Training Loss: 0.053887...</span><br><span class="line">Epoch: 12/20... Step: 150/200... Training Loss: 0.053978...</span><br><span class="line">Epoch: 12/20... Step: 175/200... Training Loss: 0.054756...</span><br><span class="line">Epoch: 12/20... Step: 200/200... Training Loss: 0.054330...</span><br><span class="line">Epoch: 12/20... Val Loss: 0.814192 Correct Rate: 80.88% (4044/5000)</span><br><span class="line">epoch 12 started:</span><br><span class="line">Epoch: 13/20... Step: 25/200... Training Loss: 0.059138...</span><br><span class="line">Epoch: 13/20... Step: 50/200... Training Loss: 0.046902...</span><br><span class="line">Epoch: 13/20... Step: 75/200... Training Loss: 0.043798...</span><br><span class="line">Epoch: 13/20... Step: 100/200... Training Loss: 0.042535...</span><br><span class="line">Epoch: 13/20... Step: 125/200... Training Loss: 0.039976...</span><br><span class="line">Epoch: 13/20... Step: 150/200... Training Loss: 0.039407...</span><br><span class="line">Epoch: 13/20... Step: 175/200... Training Loss: 0.040097...</span><br><span class="line">Epoch: 13/20... Step: 200/200... Training Loss: 0.039164...</span><br><span class="line">Epoch: 13/20... Val Loss: 0.912941 Correct Rate: 80.36% (4018/5000)</span><br><span class="line">epoch 13 started:</span><br><span class="line">Epoch: 14/20... Step: 25/200... Training Loss: 0.034176...</span><br><span class="line">Epoch: 14/20... Step: 50/200... Training Loss: 0.026745...</span><br><span class="line">Epoch: 14/20... Step: 75/200... Training Loss: 0.024255...</span><br><span class="line">Epoch: 14/20... Step: 100/200... Training Loss: 0.025095...</span><br><span class="line">Epoch: 14/20... Step: 125/200... Training Loss: 0.026316...</span><br><span class="line">Epoch: 14/20... Step: 150/200... Training Loss: 0.026680...</span><br><span class="line">Epoch: 14/20... Step: 175/200... Training Loss: 0.027141...</span><br><span class="line">Epoch: 14/20... Step: 200/200... Training Loss: 0.026514...</span><br><span class="line">Epoch: 14/20... Val Loss: 0.960679 Correct Rate: 80.5% (4025/5000)</span><br><span class="line">epoch 14 started:</span><br><span class="line">Epoch: 15/20... Step: 25/200... Training Loss: 0.034513...</span><br><span class="line">Epoch: 15/20... Step: 50/200... Training Loss: 0.030755...</span><br><span class="line">Epoch: 15/20... Step: 75/200... Training Loss: 0.029459...</span><br><span class="line">Epoch: 15/20... Step: 100/200... Training Loss: 0.030625...</span><br><span class="line">Epoch: 15/20... Step: 125/200... Training Loss: 0.029195...</span><br><span class="line">Epoch: 15/20... Step: 150/200... Training Loss: 0.028998...</span><br><span class="line">Epoch: 15/20... Step: 175/200... Training Loss: 0.027516...</span><br><span class="line">Epoch: 15/20... Step: 200/200... Training Loss: 0.030687...</span><br><span class="line">Epoch: 15/20... Val Loss: 0.838072 Correct Rate: 80.74% (4037/5000)</span><br><span class="line">epoch 15 started:</span><br><span class="line">Epoch: 16/20... Step: 25/200... Training Loss: 0.028214...</span><br><span class="line">Epoch: 16/20... Step: 50/200... Training Loss: 0.024526...</span><br><span class="line">Epoch: 16/20... Step: 75/200... Training Loss: 0.028954...</span><br><span class="line">Epoch: 16/20... Step: 100/200... Training Loss: 0.028119...</span><br><span class="line">Epoch: 16/20... Step: 125/200... Training Loss: 0.029949...</span><br><span class="line">Epoch: 16/20... Step: 150/200... Training Loss: 0.029768...</span><br><span class="line">Epoch: 16/20... Step: 175/200... Training Loss: 0.030681...</span><br><span class="line">Epoch: 16/20... Step: 200/200... Training Loss: 0.031420...</span><br><span class="line">Epoch: 16/20... Val Loss: 0.964739 Correct Rate: 80.24% (4012/5000)</span><br><span class="line">epoch 16 started:</span><br><span class="line">Epoch: 17/20... Step: 25/200... Training Loss: 0.042721...</span><br><span class="line">Epoch: 17/20... Step: 50/200... Training Loss: 0.049387...</span><br><span class="line">Epoch: 17/20... Step: 75/200... Training Loss: 0.043056...</span><br><span class="line">Epoch: 17/20... Step: 100/200... Training Loss: 0.039523...</span><br><span class="line">Epoch: 17/20... Step: 125/200... Training Loss: 0.036414...</span><br><span class="line">Epoch: 17/20... Step: 150/200... Training Loss: 0.036371...</span><br><span class="line">Epoch: 17/20... Step: 175/200... Training Loss: 0.036679...</span><br><span class="line">Epoch: 17/20... Step: 200/200... Training Loss: 0.036759...</span><br><span class="line">Epoch: 17/20... Val Loss: 0.949455 Correct Rate: 80.26% (4013/5000)</span><br><span class="line">epoch 17 started:</span><br><span class="line">Epoch: 18/20... Step: 25/200... Training Loss: 0.018441...</span><br><span class="line">Epoch: 18/20... Step: 50/200... Training Loss: 0.019064...</span><br><span class="line">Epoch: 18/20... Step: 75/200... Training Loss: 0.019507...</span><br><span class="line">Epoch: 18/20... Step: 100/200... Training Loss: 0.018219...</span><br><span class="line">Epoch: 18/20... Step: 125/200... Training Loss: 0.019207...</span><br><span class="line">Epoch: 18/20... Step: 150/200... Training Loss: 0.019247...</span><br><span class="line">Epoch: 18/20... Step: 175/200... Training Loss: 0.018839...</span><br><span class="line">Epoch: 18/20... Step: 200/200... Training Loss: 0.019666...</span><br><span class="line">Epoch: 18/20... Val Loss: 0.943150 Correct Rate: 79.72% (3986/5000)</span><br><span class="line">epoch 18 started:</span><br><span class="line">Epoch: 19/20... Step: 25/200... Training Loss: 0.020649...</span><br><span class="line">Epoch: 19/20... Step: 50/200... Training Loss: 0.016300...</span><br><span class="line">Epoch: 19/20... Step: 75/200... Training Loss: 0.014235...</span><br><span class="line">Epoch: 19/20... Step: 100/200... Training Loss: 0.014780...</span><br><span class="line">Epoch: 19/20... Step: 125/200... Training Loss: 0.013257...</span><br><span class="line">Epoch: 19/20... Step: 150/200... Training Loss: 0.013631...</span><br><span class="line">Epoch: 19/20... Step: 175/200... Training Loss: 0.012942...</span><br><span class="line">Epoch: 19/20... Step: 200/200... Training Loss: 0.013414...</span><br><span class="line">Epoch: 19/20... Val Loss: 1.100635 Correct Rate: 79.88% (3994/5000)</span><br><span class="line">epoch 19 started:</span><br><span class="line">Epoch: 20/20... Step: 25/200... Training Loss: 0.008154...</span><br><span class="line">Epoch: 20/20... Step: 50/200... Training Loss: 0.008226...</span><br><span class="line">Epoch: 20/20... Step: 75/200... Training Loss: 0.007123...</span><br><span class="line">Epoch: 20/20... Step: 100/200... Training Loss: 0.010944...</span><br><span class="line">Epoch: 20/20... Step: 125/200... Training Loss: 0.012169...</span><br><span class="line">Epoch: 20/20... Step: 150/200... Training Loss: 0.012562...</span><br><span class="line">Epoch: 20/20... Step: 175/200... Training Loss: 0.011691...</span><br><span class="line">Epoch: 20/20... Step: 200/200... Training Loss: 0.011632...</span><br><span class="line">Epoch: 20/20... Val Loss: 1.098958 Correct Rate: 80.52% (4026/5000)</span><br></pre></td></tr></table></figure>
<p><img src="https://img.nga.178.com/attachments/mon_202008/29/f0Q5-7uniZaT3cSsg-fz.jpg" alt="藤原千花想当首相"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/03/2020-09-03-rnn-movie-review-sentiment-classification/" data-id="ckew68cj5006zbstmwgnjujhw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/09/09/2020-09-09-bls-sig/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          BLS签名
        
      </div>
    </a>
  
  
    <a href="/2020/09/01/2020-09-01-hlf-simulation-8/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hyperledger Fabric-模拟脚本搭建（八）新组织通道加入</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/刷题记录/">刷题记录</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/区块链/">区块链</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/密码学/">密码学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/根域名/">根域名</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DHT/">DHT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNS/">DNS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/acm/">acm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/android/">android</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/arch/">arch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blockstack/">blockstack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/codeforces/">codeforces</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ctf/">ctf</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/fabric/">fabric</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go/">go</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/namecoin/">namecoin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node-js/">node.js</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/">ssh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/以太坊/">以太坊</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/刷题/">刷题</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/刷题记录/">刷题记录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/区块链/">区块链</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/实验/">实验</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/密码学/">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数学/">数学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/智能合约/">智能合约</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/根域名/">根域名</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率论/">概率论</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/比特币/">比特币</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/源码/">源码</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/漏洞/">漏洞</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/编程语言/">编程语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/联盟链/">联盟链</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/虚拟机/">虚拟机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/超级账本/">超级账本</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/DHT/" style="font-size: 10px;">DHT</a> <a href="/tags/DNS/" style="font-size: 15.83px;">DNS</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/RNN/" style="font-size: 10.83px;">RNN</a> <a href="/tags/acm/" style="font-size: 10.83px;">acm</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/arch/" style="font-size: 13.33px;">arch</a> <a href="/tags/blockstack/" style="font-size: 12.5px;">blockstack</a> <a href="/tags/codeforces/" style="font-size: 10px;">codeforces</a> <a href="/tags/ctf/" style="font-size: 10px;">ctf</a> <a href="/tags/docker/" style="font-size: 11.67px;">docker</a> <a href="/tags/fabric/" style="font-size: 12.5px;">fabric</a> <a href="/tags/github/" style="font-size: 11.67px;">github</a> <a href="/tags/go/" style="font-size: 10.83px;">go</a> <a href="/tags/leetcode/" style="font-size: 10.83px;">leetcode</a> <a href="/tags/linux/" style="font-size: 19.17px;">linux</a> <a href="/tags/namecoin/" style="font-size: 12.5px;">namecoin</a> <a href="/tags/node-js/" style="font-size: 11.67px;">node.js</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/ssh/" style="font-size: 10.83px;">ssh</a> <a href="/tags/zookeeper/" style="font-size: 10px;">zookeeper</a> <a href="/tags/以太坊/" style="font-size: 16.67px;">以太坊</a> <a href="/tags/刷题/" style="font-size: 12.5px;">刷题</a> <a href="/tags/刷题记录/" style="font-size: 10px;">刷题记录</a> <a href="/tags/区块链/" style="font-size: 20px;">区块链</a> <a href="/tags/大数据/" style="font-size: 10.83px;">大数据</a> <a href="/tags/实验/" style="font-size: 12.5px;">实验</a> <a href="/tags/密码学/" style="font-size: 13.33px;">密码学</a> <a href="/tags/数学/" style="font-size: 13.33px;">数学</a> <a href="/tags/智能合约/" style="font-size: 11.67px;">智能合约</a> <a href="/tags/机器学习/" style="font-size: 18.33px;">机器学习</a> <a href="/tags/根域名/" style="font-size: 14.17px;">根域名</a> <a href="/tags/概率论/" style="font-size: 10.83px;">概率论</a> <a href="/tags/比特币/" style="font-size: 11.67px;">比特币</a> <a href="/tags/深度学习/" style="font-size: 13.33px;">深度学习</a> <a href="/tags/源码/" style="font-size: 10.83px;">源码</a> <a href="/tags/漏洞/" style="font-size: 10px;">漏洞</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/编程语言/" style="font-size: 10.83px;">编程语言</a> <a href="/tags/联盟链/" style="font-size: 12.5px;">联盟链</a> <a href="/tags/虚拟机/" style="font-size: 10px;">虚拟机</a> <a href="/tags/超级账本/" style="font-size: 17.5px;">超级账本</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/09/2020-09-09-bls-sig/">BLS签名</a>
          </li>
        
          <li>
            <a href="/2020/09/03/2020-09-03-rnn-movie-review-sentiment-classification/">RNN-电影评论情感分析</a>
          </li>
        
          <li>
            <a href="/2020/09/01/2020-09-01-hlf-simulation-8/">Hyperledger Fabric-模拟脚本搭建（八）新组织通道加入</a>
          </li>
        
          <li>
            <a href="/2020/09/01/2020-09-01-hlf-simulation-7/">Hyperledger Fabric-模拟脚本搭建（七）新组织区块生成及提交</a>
          </li>
        
          <li>
            <a href="/2020/09/01/2020-09-01-hlf-simulation-6/">Hyperledger Fabric-模拟脚本搭建（六）新组织生成</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Renne<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>